{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb05eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized for crawling Xcite for device prices\n",
    "\n",
    "\n",
    "# Dependencies\n",
    "\n",
    "import html\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "# --- Menu crawling ---\n",
    "\n",
    "\n",
    "# Obtains links from Xcite's navbar\n",
    "\n",
    "def menu_crawl(main_URL, menu_items_list, menu_button) -> list:\n",
    "\n",
    "    # Implement later: check if installed; if so, just declare; if not, install and declare\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(main_URL)\n",
    "\n",
    "    HTML_master_block = []\n",
    "    \n",
    "    # Open menu\n",
    "    dropdown_menu = driver.find_element_by_tag_name(menu_button)\n",
    "    time.sleep(1)\n",
    "    dropdown_menu.click()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Crawl through menu by h5 header and snapshot each page\n",
    "    for category in menu_items_list:\n",
    "        carrot = driver.find_element(By.XPATH, f\"//h5[text()='{category}']\")\n",
    "        time.sleep(1)\n",
    "        carrot.click()\n",
    "        time.sleep(1)\n",
    "        dropdown_menu.click()\n",
    "        html_content = driver.page_source\n",
    "        HTML_master_block.append(str(BeautifulSoup(html.unescape(html_content), 'html.parser')))\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "    HTML_master = ' '.join(HTML_master_block)\n",
    "    \n",
    "    pattern = r'\"href\":\"\\/([^\"]+)\"'  # Regular expression pattern\n",
    "    matches = re.findall(pattern, HTML_master)\n",
    "    HTML_list = list(set(matches))\n",
    "    \n",
    "    return HTML_list\n",
    "\n",
    "\n",
    "# --- Product parsing ---\n",
    "\n",
    "# Backend\n",
    "\n",
    "\n",
    "# Filters graves matching Xcite's product class inheritance\n",
    "\n",
    "def filter_graves(graveyard) -> list:\n",
    "    \n",
    "    # Step 2: Function to filter out only graves conforming to Xcite products class inheritance (ProductList_tileWrapper__cV7B_)\n",
    "    filtered_graves = []\n",
    "    for grave in graveyard:\n",
    "        if 'ProductList_tileWrapper__cV7B_' in grave:\n",
    "            filtered_graves.append(f\"<li class{grave}\")\n",
    "\n",
    "    return filtered_graves\n",
    "\n",
    "\n",
    "# Functional\n",
    "\n",
    "\n",
    "# Creates a grave for a specific page\n",
    "\n",
    "def grave_list(crawl_URL):\n",
    "    \n",
    "    # Step 1: Pull page source and split into list items (products) graves; return as a grave list\n",
    "    random_time = round(random.uniform(1, 2),2)\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(crawl_URL)\n",
    "    time.sleep(random_time)\n",
    "    html_content = driver.page_source\n",
    "    driver.quit()\n",
    "    list_graves = str(BeautifulSoup(html.unescape(html_content), 'html.parser')).split('<li class')\n",
    "    \n",
    "    return list_graves\n",
    "\n",
    "\n",
    "# Crawls through a list of links and creates product class filtered graves for each webpage: HEAVY OPERATION <ONLY RUN WHEN 100% SURE>\n",
    "\n",
    "def link_crawl(listed_links) -> list(list()):\n",
    "    \n",
    "    # Step 3: For each link in the Xcite_links_filtered list, perform grave_list splitting, filtering, and store in graveyard\n",
    "    filtered_graveyard = []\n",
    "    for link in listed_links:\n",
    "        filtered_graveyard.append(filter_graves(grave_list(link)))\n",
    "    \n",
    "    return filtered_graveyard\n",
    "\n",
    "\n",
    "# Crawls through a graveyard of product class filtered graves and parses product information into a product dictionary\n",
    "\n",
    "def cemetary_product_parse(filtered_cemetary) -> dict:\n",
    "    \n",
    "    # Step 4: Parse information to build the product dictionary\n",
    "    product_dictionary = {}\n",
    "    \n",
    "    for graveyard in filtered_cemetary:\n",
    "        for filtered_grave in graveyard:\n",
    "\n",
    "            soup = BeautifulSoup(filtered_grave, 'html.parser')\n",
    "\n",
    "            # product_name\n",
    "            p_tag = soup.find('p')\n",
    "            p_text = p_tag.get_text(strip=True)\n",
    "\n",
    "            product_dictionary[p_text] = {\n",
    "                'product_brand':'', \n",
    "                'product_price':'', \n",
    "                'product_discount':'', \n",
    "                'price_before_discount':'', \n",
    "                'product_link':'', \n",
    "                'product_image':''\n",
    "            }\n",
    "\n",
    "            # product_brand\n",
    "            try:\n",
    "                h5_tag = soup.find('h5')\n",
    "                h5_text = h5_tag.get_text(strip=True)\n",
    "                product_dictionary[p_text]['product_brand'] = h5_text\n",
    "            except Exception as error:\n",
    "                print(f\"No info because: {error}\")\n",
    "\n",
    "            # product_price\n",
    "            try:\n",
    "                span_tag = soup.find('span', 'text-2xl text-functional-red-800 block mb-2')\n",
    "                span_text = span_tag.get_text(strip=True)\n",
    "                product_dictionary[p_text]['product_price'] = span_text\n",
    "            except Exception as error:\n",
    "                try:\n",
    "                    h4_tag = soup.find('h4')\n",
    "                    h4_text = h4_tag.get_text(strip=True)\n",
    "                    product_dictionary[p_text]['product_price'] = h4_text\n",
    "                except Exception as error:\n",
    "                    print(f\"No info because: {error}\")\n",
    "\n",
    "\n",
    "            # product_discount\n",
    "            try:\n",
    "                span_tag2 = soup.find('span', 'text-base bg-functional-red-600 text-white px-2 py-[3px] leading-1 align-text-top inline-block font-normal')\n",
    "                span2_text = span_tag2.get_text(strip=True)\n",
    "                product_dictionary[p_text]['product_discount'] = span2_text\n",
    "            except Exception as error:\n",
    "                print(f\"No info because: {error}\")\n",
    "\n",
    "            # price_before_discount\n",
    "            try:\n",
    "                span_tag3 = soup.find('span', 'text-base line-through')\n",
    "                span3_text = span_tag3.get_text(strip=True)\n",
    "                product_dictionary[p_text]['price_before_discount'] = span3_text\n",
    "            except Exception as error:\n",
    "                print(f\"No info because: {error}\")\n",
    "\n",
    "            # product_link\n",
    "            try:\n",
    "                a_tag = soup.find('a')\n",
    "                href_link = a_tag['href']\n",
    "                product_dictionary[p_text]['product_link'] = href_link\n",
    "            except Exception as error:\n",
    "                print(f\"No info because: {error}\")\n",
    "\n",
    "            # product_image\n",
    "            try:\n",
    "                img_tags = soup.find_all('img')\n",
    "                if len(img_tags) >= 2:\n",
    "                    img_tag2 = img_tags[1]\n",
    "                src_link = img_tag2['src']\n",
    "                product_dictionary[p_text]['product_image'] = src_link\n",
    "            except Exception as error:\n",
    "                print(f\"No info because: {error}\")\n",
    "\n",
    "    return product_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be629d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded. Print available functions? [Y]\n",
      "y\n",
      "\n",
      "    menu_crawl(main_URL, menu_items_list, menu_button)\n",
      "    \n",
      "    def grave_list(crawl_URL)\n",
      "    \n",
      "    link_crawl(listed_links)\n",
      "    \n",
      "    cemetary_product_parse(filtered_cemetary)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "\n",
    "print(\"Successfully loaded. Print available functions? [Y]\")\n",
    "response = input()\n",
    "if response in ['Y', 'y']:\n",
    "    print(\"\"\"\n",
    "    menu_crawl(main_URL, menu_items_list, menu_button)\n",
    "    \n",
    "    def grave_list(crawl_URL)\n",
    "    \n",
    "    link_crawl(listed_links)\n",
    "    \n",
    "    cemetary_product_parse(filtered_cemetary)\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5d4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
